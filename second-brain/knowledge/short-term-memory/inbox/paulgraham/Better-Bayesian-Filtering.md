---
title: "Better Bayesian Filtering"
author: Paul Graham
source: http://paulgraham.com/better.html
year_published: Unknown
date_scraped: 2025-08-07
time_scraped: 14:08:46
word_count: 3978
scrape_method: firecrawl
tags: [paul-graham, essays]
---

# Better Bayesian Filtering

|     |     |     |
| --- | --- | --- |
| ![](https://s.turbifycdn.com/aah/paulgraham/essays-5.gif) | ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) | [![](https://s.turbifycdn.com/aah/paulgraham/essays-6.gif)](https://paulgraham.com/index.html)

|     |
| --- |
| ![Better Bayesian Filtering](https://s.turbifycdn.com/aah/paulgraham/better-bayesian-filtering-2.gif)<br>January 2003<br>_(This article was given as a talk at the 2003 Spam Conference._<br>_It describes the work I've done to improve the performance of_<br>_the algorithm described in [A Plan for Spam](https://paulgraham.com/spam.html),_<br>_and what I plan to do in the future.)_<br>The first discovery I'd like to present here is an algorithm for<br>lazy evaluation of research papers. Just<br>write whatever you want and don't cite any previous work, and<br>indignant readers will send you references to all the papers you<br>should have cited. I discovered this algorithm<br>after \`\`A Plan for Spam'' \[1\] was on Slashdot.<br>Spam filtering is a subset of text classification,<br>which is a well established field, but the first papers about<br>Bayesian<br>spam filtering per se seem to have been two<br>given at the same conference in 1998,<br>one by Pantel and Lin \[2\],<br>and another by a group from<br>Microsoft Research \[3\].<br>When I heard about this work I was a bit surprised. If<br>people had been onto Bayesian filtering four years ago,<br>why wasn't everyone using it?<br>When I read the papers I found out why. Pantel and Lin's filter was the<br>more effective of the two, but it<br>only caught 92% of spam, with 1.16% false positives.<br>When I tried writing a Bayesian spam filter,<br>it caught 99.5% of spam with less than .03% false<br>positives \[4\].<br>It's always alarming when two people<br>trying the same experiment get widely divergent results.<br>It's especially alarming here because those two sets of numbers<br>might yield opposite conclusions.<br>Different users have different requirements, but I think for<br>many people a filtering rate of 92% with 1.16% false positives means<br>that filtering is not an acceptable solution, whereas<br>99.5% with less than .03% false positives means that it is.<br>So why did we get such different numbers?<br>I haven't tried to reproduce Pantel and Lin's results, but<br>from reading the paper I see five things that probably account<br>for the difference.<br>One is simply that they trained their filter on very little<br>data: 160 spam and 466 nonspam mails.<br>Filter performance should still be climbing with data<br>sets that small. So their numbers may not even be an accurate<br>measure of the performance of their algorithm, let alone of<br>Bayesian spam filtering in general.<br>But I think the most important difference is probably<br>that they ignored message headers. To anyone who has worked<br>on spam filters, this will seem a perverse decision.<br>And yet in the very first filters I tried writing, I ignored the<br>headers too. Why? Because I wanted to keep the problem neat.<br>I didn't know much about mail headers then, and they seemed to me<br>full of random stuff. There is a lesson here for filter<br>writers: don't ignore data. You'd think this lesson would<br>be too obvious to mention, but I've had to learn it several times.<br>Third, Pantel and Lin stemmed the tokens, meaning they reduced e.g. both<br>\`\`mailing'' and \`\`mailed'' to the root \`\`mail''. They may<br>have felt they were forced to do this by the small size<br>of their corpus, but if so this is a kind of premature <br>optimization.<br>Fourth, they calculated probabilities differently.<br>They used all the tokens, whereas I only<br>use the 15 most significant. If you use all the tokens<br>you'll tend to miss longer spams, the type where someone tells you their life<br>story up to the point where they got rich from some multilevel<br>marketing scheme. And such an algorithm<br>would be easy for spammers to spoof: just add a big<br>chunk of random text to counterbalance the spam terms.<br>Finally, they didn't bias against false positives.<br>I think<br>any spam filtering algorithm ought to have a convenient<br>knob you can twist to decrease the<br>false positive rate at the expense of the filtering rate.<br>I do this by counting the occurrences<br>of tokens in the nonspam corpus double. <br>I don't think it's a good idea to treat spam filtering as<br>a straight text classification problem. You can use<br>text classification techniques, but solutions can and should<br>reflect the fact that the text is email, and spam<br>in particular. Email is not just text; it has structure.<br>Spam filtering is not just classification, because<br>false positives are so much worse than false negatives<br>that you should treat them as a different kind of error.<br>And the source of error is not just random variation, but<br>a live human spammer working actively to defeat your filter.<br>**Tokens**<br>Another project I heard about<br>after the Slashdot article was Bill Yerazunis' <br>[CRM114](http://crm114.sourceforge.net/) \[5\].<br>This is the counterexample to the design principle I<br>just mentioned. It's a straight text classifier,<br>but such a stunningly effective one that it manages to filter<br>spam almost perfectly without even knowing that's<br>what it's doing.<br>Once I understood how CRM114 worked, it seemed<br>inevitable that I would eventually have to move from filtering based<br>on single words to an approach like this. But first, I thought,<br>I'll see how far I can get with single words. And the answer is,<br>surprisingly far.<br>Mostly I've been working on smarter tokenization. On<br>current spam, I've been able to achieve filtering rates that<br>approach CRM114's. These techniques are mostly orthogonal to Bill's;<br>an optimal solution might incorporate both.<br>\`\`A Plan for Spam'' uses a very simple<br>definition of a token. Letters, digits, dashes, apostrophes,<br>and dollar signs are constituent characters, and everything<br>else is a token separator. I also ignored case.<br>Now I have a more complicated definition of a token:<br>1. Case is preserved.<br>   <br>2. Exclamation points are constituent characters.<br>   <br>3. Periods and commas are constituents if they occur<br>    between two digits. This lets me get ip addresses<br>    and prices intact.<br>   <br>4. A price range like $20-25 yields two tokens,<br>    $20 and $25.<br>   <br>5. Tokens that occur within the<br>    To, From, Subject, and Return-Path lines, or within urls,<br>    get marked accordingly. E.g. \`\`foo'' in the Subject line<br>    becomes \`\`Subject\*foo''. (The asterisk could<br>    be any character you don't allow as a constituent.)<br>   <br>Such measures increase the filter's vocabulary, which<br>makes it more discriminating. For example, in the current<br>filter, \`\`free'' in the Subject line<br>has a spam probability of 98%, whereas the same token<br>in the body has a spam probability of only 65%.<br>Here are some of the current probabilities \[6\]:<br>```<br>Subject*FREE      0.9999<br>free!!            0.9999<br>To*free           0.9998<br>Subject*free      0.9782<br>free!             0.9199<br>Free              0.9198<br>Url*free          0.9091<br>FREE              0.8747<br>From*free         0.7636<br>free              0.6546<br>```<br>In the Plan for Spam filter, all these tokens would have had the<br>same probability, .7602. That filter recognized about 23,000<br>tokens. The current one recognizes about 187,000.<br>The disadvantage of having a larger universe of tokens<br>is that there is more<br>chance of misses.<br>Spreading your corpus out over more tokens<br>has the same effect as making it smaller.<br>If you consider exclamation points as<br>constituents, for example, then you could end up<br>not having a spam probability for free with seven exclamation<br>points, even though you know that free with just two <br>exclamation points has a probability of 99.99%.<br>One solution to this is what I call degeneration. If you<br>can't find an exact match for a token,<br>treat it as if it were a less specific<br>version. I consider terminal exclamation<br>points, uppercase letters, and occurring in one of the<br>five marked contexts as making a token more specific.<br>For example, if I don't find a probability for<br>\`\`Subject\*free!'', I look for probabilities for<br>\`\`Subject\*free'', \`\`free!'', and \`\`free'', and take whichever one<br>is farthest from .5.<br>Here are the alternatives \[7\]<br>considered if the filter sees \`\`FREE!!!'' in the<br>Subject line and doesn't have a probability for it.<br>```<br>Subject*Free!!!<br>Subject*free!!!<br>Subject*FREE!<br>Subject*Free!<br>Subject*free!<br>Subject*FREE<br>Subject*Free<br>Subject*free<br>FREE!!!<br>Free!!!<br>free!!!<br>FREE!<br>Free!<br>free!<br>FREE<br>Free<br>free              <br>```<br>If you do this, be sure to consider versions with initial<br>caps as well as all uppercase and all lowercase. Spams<br>tend to have more sentences in imperative mood, and in<br>those the first word is a verb. So verbs with initial caps<br>have higher spam probabilities than they would in all <br>lowercase. In my filter, the spam probability of \`\`Act''<br>is 98% and for \`\`act'' only 62%.<br>If you increase your filter's vocabulary, you can end up<br>counting the same word multiple times, according to your old<br>definition of \`\`same''.<br>Logically, they're not the<br>same token anymore. But if this still bothers you, let<br>me add from experience that the words you seem to be<br>counting multiple times tend to be exactly the ones you'd<br>want to.<br>Another effect of a larger vocabulary is that when you<br>look at an incoming mail you find more interesting tokens,<br>meaning those with probabilities far from .5. I use the<br>15 most interesting to decide if mail is spam.<br>But you can run into a problem when you use a fixed number<br>like this. If you find a lot of maximally interesting tokens,<br>the result can end up being decided by whatever random factor<br>determines the ordering of equally interesting tokens.<br>One way to deal with this is to treat some<br>as more interesting than others.<br>For example, the<br>token \`\`dalco'' occurs 3 times in my spam corpus and never<br>in my legitimate corpus. The token \`\`Url\*optmails''<br>(meaning \`\`optmails'' within a url) occurs 1223 times.<br>And yet, as I used to calculate probabilities for tokens,<br>both would have the same spam probability, the threshold of .99.<br>That doesn't feel right. There are theoretical<br>arguments for giving these two tokens substantially different<br>probabilities (Pantel and Lin do), but I haven't tried that yet.<br>It does seem at least that if we find more than 15 tokens<br>that only occur in one corpus or the other, we ought to<br>give priority to the ones that occur a lot. So now<br>there are two threshold values. For tokens that occur only<br>in the spam corpus, the probability is .9999 if they<br>occur more than 10 times and .9998 otherwise. Ditto<br>at the other end of the scale for tokens found<br>only in the legitimate corpus.<br>I may later scale token probabilities substantially,<br>but this tiny amount of scaling at least ensures that <br>tokens get sorted the right way.<br>Another possibility would be to consider not<br>just 15 tokens, but all the tokens over a certain<br>threshold of interestingness. Steven Hauser does this<br>in his statistical spam filter \[8\].<br>If you use a threshold, make it very high, or<br>spammers could spoof you by packing messages with<br>more innocent words.<br>Finally, what should one do<br>about html? I've tried the whole spectrum of options, from<br>ignoring it to parsing it all. Ignoring html is a bad idea,<br>because it's full of useful spam signs. But if you parse <br>it all, your filter might degenerate into a mere html <br>recognizer. The most effective approach<br>seems to be the middle course, to notice some tokens but not<br>others. I look at a, img, and font tags, and ignore the<br>rest. Links and images you should certainly look at, because<br>they contain urls.<br>I could probably be smarter about dealing with html, but I<br>don't think it's worth putting a lot of time into this.<br>Spams full of html are easy to filter. The smarter<br>spammers already avoid it. So<br>performance in the future should not depend much on how<br>you deal with html.<br>**Performance**<br>Between December 10 2002 and January 10 2003 I got about<br>1750 spams. <br>Of these, 4 got through. That's a filtering<br>rate of about 99.75%.<br>Two of the four spams I missed got through because they<br>happened to use words that occur often in my legitimate<br>email.<br>The third was one of those that exploit<br>an insecure cgi script to send mail to third parties.<br>They're hard to filter based just<br>on the content because the headers are innocent and <br>they're careful about the words they use. Even so I can<br>usually catch them. This one squeaked by with a<br>probability of .88, just under the threshold of .9.<br>Of course, looking at multiple token sequences<br>would catch it easily. \`\`Below is the result of<br>your feedback form'' is an instant giveaway.<br>The fourth spam was what I call<br>a spam-of-the-future, because this is what I expect spam to<br>evolve into: some completely neutral<br>text followed by a url. In this case it was was from<br>someone saying they had finally finished their homepage<br>and would I go look at it. (The page was of course an <br>ad for a porn site.)<br>If the spammers are careful about the headers and use a<br>fresh url, there is nothing in spam-of-the-future for filters<br>to notice. We can of course counter by sending a<br>crawler to look at the page. But that might not be necessary.<br>The response rate for spam-of-the-future must<br>be low, or everyone would be doing it.<br>If it's low enough,<br>it [won't pay](https://paulgraham.com/wfks.html) for spammers to send it, and we won't <br>have to work too hard on filtering it.<br>Now for the really shocking news: during that same one-month<br>period I got _three_ false positives.<br>In a way it's<br>a relief to get some false positives. When I wrote \`\`A Plan<br>for Spam'' I hadn't had any, and I didn't know what they'd<br>be like. Now that I've had a few, I'm relieved to find<br>they're not as bad as I feared.<br>False positives yielded by statistical<br>filters turn out to be mails that sound a lot like spam, and<br>these tend to be the ones you would least mind missing \[9\].<br>Two of the false positives were newsletters<br>from companies I've bought things from. I never<br>asked to receive them, so arguably they<br>were spams, but I count them as false positives because<br>I hadn't been deleting them as spams before. The reason<br>the filters caught them was that both companies in <br>January switched to commercial email senders<br>instead of sending the mails from their own servers, <br>and both the headers and the bodies became much spammier.<br>The third false positive was a bad one, though. It was <br>from someone in Egypt and written in all uppercase. This was<br>a direct result of making tokens case sensitive; the Plan<br>for Spam filter wouldn't have caught it.<br>It's hard to say what the overall false positive rate is,<br>because we're up in the noise, statistically.<br>Anyone who has worked on filters (at least, effective filters) will<br>be aware of this problem.<br>With some emails it's<br>hard to say whether they're spam or not, and these are<br>the ones you end up looking at when you get filters <br>really tight. For example, so far the filter has<br>caught two emails that were sent to my address because<br>of a typo, and one sent to me in the belief that I was <br>someone else. Arguably, these are neither my spam<br>nor my nonspam mail.<br>Another false positive was from a vice president at Virtumundo.<br>I wrote to them pretending to be a customer,<br>and since the reply came back through Virtumundo's <br>mail servers it had the most incriminating<br>headers imaginable. Arguably this isn't a real false<br>positive either, but a sort of Heisenberg uncertainty<br>effect: I only got it because I was writing about spam <br>filtering.<br>Not counting these, I've had a total of five false positives<br>so far, out of about 7740 legitimate emails, a rate of .06%.<br>The other two were a notice that something I bought<br>was back-ordered, and a party reminder from Evite.<br>I don't think this number can be trusted, partly<br>because the sample is so small, and partly because<br>I think I can fix the filter not to catch<br>some of these.<br>False positives seem to me a different kind of error from<br>false negatives.<br>Filtering rate is a measure of performance. False<br>positives I consider more like bugs. I approach improving the<br>filtering rate as optimization, and decreasing false<br>positives as debugging.<br>So these five false positives are my bug list. For example, <br>the mail from Egypt got nailed because the uppercase text<br>made it look to the filter like a Nigerian spam.<br>This really is kind of a bug. As with<br>html, the email being all uppercase is really conceptually _one_<br>feature, not one for each word. I need to handle case in a<br>more sophisticated way.<br>So what to make of this .06%? Not much, I think. You could<br>treat it as an upper bound, bearing in mind the small sample size.<br>But at this stage it is more a measure of the bugs<br>in my implementation than some intrinsic false positive rate<br>of Bayesian filtering.<br>**Future**<br>What next? Filtering is an optimization problem,<br>and the key to optimization is profiling. Don't<br>try to guess where your code is slow, because you'll<br>guess wrong. _Look_ at where your code is slow,<br>and fix that. In filtering, this translates to: <br>look at the spams you miss, and figure out what you<br>could have done to catch them.<br>For example, spammers are now working aggressively to <br>evade filters, and one of the things they're doing is<br>breaking up and misspelling words to prevent filters from<br>recognizing them. But working on this is not my first<br>priority, because I still have no trouble catching these<br>spams \[10\].<br>There are two kinds of spams I currently do<br>have trouble with.<br>One is the type that pretends to be an email from <br>a woman inviting you to go chat with her or see her profile on a dating<br>site. These get through because they're the one type of<br>sales pitch you can make without using sales talk. They use<br>the same vocabulary as ordinary email.<br>The other kind of spams I have trouble filtering are those<br>from companies in e.g. Bulgaria offering contract programming <br>services. These get through because I'm a programmer too, and<br>the spams are full of the same words as my real mail.<br>I'll probably focus on the personal ad type first. I think if<br>I look closer I'll be able to find statistical differences<br>between these and my real mail. The style of writing is<br>certainly different, though it may take multiword filtering<br>to catch that.<br>Also, I notice they tend to repeat the url,<br>and someone including a url in a legitimate mail wouldn't do that \[11\].<br>The outsourcing type are going to be hard to catch. Even if <br>you sent a crawler to the site, you wouldn't find a smoking<br>statistical gun.<br>Maybe the only answer is a central list of<br>domains advertised in spams \[12\]. But there can't be that<br>many of this type of mail. If the only<br>spams left were unsolicited offers of contract programming<br>services from Bulgaria, we could all probably move on to<br>working on something else.<br>Will statistical filtering actually get us to that point?<br>I don't know. Right now, for me personally, spam is<br>not a problem. But spammers haven't yet made a serious<br>effort to spoof statistical filters. What will happen when they do?<br>I'm not optimistic about filters that work at the<br>network level \[13\].<br>When there is a static obstacle worth getting past, spammers<br>are pretty efficient at getting past it. There<br>is already a company called Assurance Systems that will<br>run your mail through Spamassassin and tell you whether <br>it will get filtered out.<br>Network-level filters won't be completely useless.<br>They may be enough to kill all the "opt-in"<br>spam, meaning spam from companies like Virtumundo and<br>Equalamail who claim that they're really running opt-in lists.<br>You can filter those based just on the headers, no<br>matter what they say in the body. But anyone willing to<br>falsify headers or use open relays, presumably including<br>most porn spammers, should be able to get some message past<br>network-level filters if they want to. (By no means the<br>message they'd like to send though, which is something.)<br>The kind of filters I'm optimistic about are ones that<br>calculate probabilities based on each individual user's mail.<br>These can be much more effective, not only in<br>avoiding false positives, but in filtering too: for example,<br>finding the recipient's email address base-64 encoded anywhere in<br>a message is a very good spam indicator.<br>But the real advantage of individual filters is that they'll all be<br>different. If everyone's filters have different probabilities,<br>it will make the spammers' optimization loop, what programmers<br>would call their edit-compile-test cycle, appallingly slow. <br>Instead of just tweaking a spam till it gets through a copy of<br>some filter they have on their desktop, they'll have to do a<br>test mailing for each tweak. It would be like programming in<br>a language without an interactive toplevel, <br>and I wouldn't wish that<br>on anyone.<br>**Notes**<br>\[1\]<br>Paul Graham. \`\`A Plan for Spam.'' August 2002.<br>http://paulgraham.com/spam.html.<br>Probabilities in this algorithm are<br>calculated using a degenerate case of Bayes' Rule. There are<br>two simplifying assumptions: that the probabilities<br>of features (i.e. words) are independent, and that we know<br>nothing about the prior probability of an email being<br>spam.<br>The first assumption is widespread in text classification.<br>Algorithms that use it are called \`\`naive Bayesian.''<br>The second assumption I made because the proportion of spam in<br>my incoming mail fluctuated so much from day to day (indeed,<br>from hour to hour) that the overall prior ratio seemed<br>worthless as a predictor. If you assume that P(spam) and<br>P(nonspam) are both .5, they cancel out and you can<br>remove them from the formula.<br>If you were doing Bayesian filtering in a situation where <br>the ratio of spam to nonspam was consistently very high or<br>(especially) very low, you could probably improve filter<br>performance by incorporating prior probabilities. To do<br>this right you'd have to track ratios by time of day, because<br>spam and legitimate mail volume both have distinct daily<br>patterns.<br>\[2\]<br>Patrick Pantel and Dekang Lin. \`\`SpamCop-- A Spam<br>Classification & Organization Program.'' Proceedings of AAAI-98<br>Workshop on Learning for Text Categorization.<br>\[3\]<br>Mehran Sahami, Susan Dumais, David Heckerman and Eric Horvitz.<br>\`\`A Bayesian Approach to Filtering Junk E-Mail.'' Proceedings of AAAI-98<br>Workshop on Learning for Text Categorization.<br>\[4\] At the time I had zero false positives out of about 4,000 <br>legitimate emails. If the next legitimate email was<br>a false positive, this would give us .03%. These false positive<br>rates are untrustworthy, as I explain later. I quote<br>a number here only to emphasize that whatever the false positive rate<br>is, it is less than 1.16%.<br>\[5\] Bill Yerazunis. \`\`Sparse Binary Polynomial Hash Message<br>Filtering and The CRM114 Discriminator.'' Proceedings of 2003<br>Spam Conference.<br>\[6\] In \`\`A Plan for Spam'' I used thresholds of .99 and .01.<br>It seems justifiable to use thresholds proportionate to the<br>size of the corpora. Since I now have on the order of 10,000 of each<br>type of mail, I use .9999 and .0001.<br>\[7\] There is a flaw here I should probably fix. Currently,<br>when \`\`Subject\*foo'' degenerates to just \`\`foo'', what that means is<br>you're getting the stats for occurrences of \`\`foo'' in<br>the body or header lines other than those I mark.<br>What I should do is keep track of statistics for \`\`foo''<br>overall as well as specific versions, and degenerate from<br>\`\`Subject\*foo'' not to \`\`foo'' but to \`\`Anywhere\*foo''. Ditto for<br>case: I should degenerate from uppercase to any-case, not<br>lowercase.<br>It would probably be a win to do this with prices<br>too, e.g. to degenerate from \`\`$129.99'' to \`\`$--9.99'', \`\`$--.99'',<br>and \`\`$--''.<br>You could also degenerate from words to their stems,<br>but this would probably only improve filtering rates early on <br>when you had small corpora.<br>\[8\] Steven Hauser. \`\`Statistical Spam Filter Works for Me.''<br>http://www.sofbot.com.<br>\[9\] False positives are not all equal, and we should remember<br>this when comparing techniques for stopping spam.<br>Whereas many of the false positives caused by filters<br>will be near-spams that you wouldn't mind missing,<br>false positives caused by blacklists, for example, will be just<br>mail from people who chose the wrong ISP. In both<br>cases you catch mail that's near spam, but for blacklists nearness<br>is physical, and for filters it's textual.<br>\[10\] If spammers get good enough at obscuring tokens <br>for this to be a problem, we can respond by simply removing<br>whitespace, periods, commas, etc. and using a dictionary to<br>pick the words out of the resulting sequence.<br>And of course finding words this way that weren't visible in<br>the original text would in itself be evidence of spam.<br>Picking out the words won't be trivial. It will require <br>more than just reconstructing word boundaries; spammers<br>both add (\`\`xHot nPorn cSite'') and omit (\`\`P#rn'') letters.<br>Vision research may be useful here, since human vision is<br>the limit that such tricks will approach.<br>\[11\] <br>In general, spams are more repetitive than regular email. <br>They want to pound that message home. I currently don't<br>allow duplicates in the top 15 tokens, because<br>you could get a false positive if the sender happens to use<br>some bad word multiple times. (In my current filter, \`\`dick'' has<br>a spam probabilty of .9999, but it's also a name.)<br>It seems we should at least notice duplication though,<br>so I may try allowing up to two of each token, as Brian Burton does in<br>SpamProbe.<br>\[12\] This is what approaches like Brightmail's will<br>degenerate into once spammers are pushed into using mad-lib<br>techniques to generate everything else in the message.<br>\[13\]<br>It's sometimes argued that we should be working on filtering<br>at the network level, because it is more efficient. What people<br>usually mean when they say this is: we currently filter at the<br>network level, and we don't want to start over from scratch.<br>But you can't dictate the problem to fit your solution.<br>Historically, scarce-resource arguments have been the losing<br>side in debates about software design.<br>People only tend to use them to justify choices<br>(inaction in particular) made for other reasons.<br>**Thanks** to Sarah Harlin, Trevor Blackwell, and<br>Dan Giffin for reading drafts of this paper, and to Dan again<br>for most of the infrastructure that this filter runs on.<br>**Related:** |

|     |     |     |
| --- | --- | --- |
| ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) |
| ![](https://s.turbifycdn.com/aah/paulgraham/serious-2.gif) | ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) | [A Plan for Spam](https://paulgraham.com/spam.html)<br>![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) |
| ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) |
| ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) |
| ![](https://s.turbifycdn.com/aah/paulgraham/serious-2.gif) | ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) | [Plan for Spam FAQ](https://paulgraham.com/spamfaq.html)<br>![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) |
| ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) |
| ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) |
| ![](https://s.turbifycdn.com/aah/paulgraham/serious-2.gif) | ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) | [2003 Spam Conference Proceedings](http://spamconference.org/proceedings2003.html)<br>![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) |
| ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) |
| ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) |
| ![](https://s.turbifycdn.com/aah/paulgraham/serious-2.gif) | ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) | [Japanese Translation](http://www.shiro.dreamhost.com/scheme/trans/better-j.html)<br>![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) |
| ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) |
| ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) |
| ![](https://s.turbifycdn.com/aah/paulgraham/serious-2.gif) | ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) | [Chinese Translation](http://people.brandeis.edu/~liji/_private/translation/better.htm)<br>![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) |
| ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) |
| ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) |
| ![](https://s.turbifycdn.com/aah/paulgraham/serious-2.gif) | ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) | [Test of These Suggestions](http://www.bgl.nu/bogofilter/graham.html)<br>![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) |
| ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) |

|     |
| --- |
| * * * | |
