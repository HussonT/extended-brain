---
title: "The Hundred-Year Language"
author: Paul Graham
source: http://paulgraham.com/hundred.html
year_published: Unknown
date_scraped: 2025-08-07
time_scraped: 12:14:51
word_count: 4370
scrape_method: firecrawl
tags: [paul-graham, essays, startups, programming, philosophy]
---

# The Hundred-Year Language

|     |     |     |
| --- | --- | --- |
| ![](https://s.turbifycdn.com/aah/paulgraham/essays-5.gif) | ![](https://sep.turbifycdn.com/ca/Img/trans_1x1.gif) | [![](https://s.turbifycdn.com/aah/paulgraham/essays-6.gif)](https://paulgraham.com/index.html)

|     |
| --- |
| [![](https://s.turbifycdn.com/aah/paulgraham/the-hundred-year-language-12.gif)](https://s.turbifycdn.com/aah/paulgraham/the-hundred-year-language-11.gif)<br>![The Hundred-Year Language](https://s.turbifycdn.com/aah/paulgraham/the-hundred-year-language-13.gif)<br>April 2003<br>_(This essay is derived from a keynote talk at PyCon 2003.)_<br>It's hard to predict what<br>life will be like in a hundred years. There are only a few<br>things we can say with certainty. We know that everyone will<br>drive flying cars,<br>that zoning laws will be relaxed to allow buildings<br>hundreds of stories tall, that it will be dark most of the<br>time, and that women will all be trained in the martial arts. <br>Here I want to zoom in on one detail of this<br>picture. What kind of programming language will they use to<br>write the software controlling those flying cars?<br>This is worth thinking about not so<br>much because we'll actually get to use these languages as because,<br>if we're lucky, we'll use languages on the path from this<br>point to that.<br>I think that, like species, languages will form evolutionary trees,<br>with dead-ends branching off all over. We can see this<br>happening already.<br>Cobol, for all its sometime popularity, does not seem to have any<br>intellectual descendants. It is an evolutionary dead-end-- a<br>Neanderthal language.<br>I predict a similar fate for Java. People<br>sometimes send me mail saying, "How can you say that Java<br>won't turn out to be a successful language? It's already<br>a successful language." And I admit that it is, if you<br>measure success by shelf space taken up by books on it<br>(particularly individual books on it), or by<br>the number of undergrads who believe they have to<br>learn it to get a job. When I say Java won't<br>turn out to be a successful language, I mean something more<br>specific: that Java<br>will turn out to be an evolutionary dead-end, like Cobol.<br>This is just a guess. I may be wrong. My point here is not to dis Java,<br>but to raise the issue of evolutionary<br>trees and get people asking, where on the tree is language X?<br>The reason to ask this question isn't just so that<br>our ghosts can say, in a<br>hundred years, I told you so. It's because staying close to <br>the main branches is a useful heuristic for finding languages that will<br>be good to program in now.<br>At any given time, you're probably happiest on<br>the main branches of an evolutionary tree.<br>Even when there were still plenty of Neanderthals, <br>it must have sucked to be one. The<br>Cro-Magnons would have been constantly coming over and<br>beating you up and stealing your food.<br>The reason I want to<br>know what languages will be like in a hundred years is so that<br>I know what branch of the tree to bet on now.<br>The evolution of languages differs from the evolution of species<br>because branches can converge. The Fortran branch, for example,<br>seems to be merging with the descendants<br>of Algol. In theory this is possible for species too, but it's<br>not likely to have happened to any bigger than a cell.<br>Convergence<br>is more likely for languages partly because the space of<br>possibilities is smaller, and partly because mutations<br>are not random. Language designers deliberately incorporate<br>ideas from other languages.<br>It's especially useful for language designers to think<br>about where the evolution of programming languages is likely<br>to lead, because they can steer accordingly. <br>In that case, "stay on a main branch" becomes more than a<br>way to choose a good language.<br>It becomes a heuristic for making the right decisions about<br>language design.<br>Any programming language can be divided into<br>two parts: some set of fundamental operators that play the role<br>of axioms, and the rest of the language, which could in principle<br>be written in terms of these fundamental operators.<br>I think the fundamental operators are the most important factor in a<br>language's long term survival. The rest you can change. It's<br>like the rule that in buying a house you should consider<br>location first of all. Everything else you can fix later, but you<br>can't fix the location.<br>I think it's important not just that the axioms be well chosen, <br>but that there be few of them. Mathematicians have always felt <br>this way about axioms-- the fewer, the better-- and I think they're<br>onto something.<br>At the very least, it has to be a useful exercise to look closely<br>at the core of a language to see if there are any axioms that<br>could be weeded out. I've found in my long career as a slob that<br>cruft breeds cruft, and I've seen this happen in software as<br>well as under beds and in the corners of rooms.<br>I have a hunch that<br>the main branches of the evolutionary tree pass through the languages<br>that have the smallest, cleanest cores.<br>The more of a language you can write in itself,<br>the better.<br>Of course, I'm making a big assumption in even asking what<br>programming languages will be like in a hundred years.<br>Will we even be writing programs in a hundred years? Won't<br>we just tell computers what we want them to do?<br>There hasn't been a lot of progress in that department<br>so far.<br>My guess is that a hundred years from now people will<br>still tell computers what to do using programs we would recognize<br>as such. There may be tasks that we<br>solve now by writing programs and which in a hundred years<br>you won't have to write programs to solve, but I think<br>there will still be a good deal of<br>programming of the type that we do today.<br>It may seem presumptuous to think anyone can predict what<br>any technology will look like in a hundred years. But<br>remember that we already have almost fifty years of history behind us.<br>Looking forward a hundred years is a graspable idea<br>when we consider how slowly languages have evolved in the<br>past fifty.<br>Languages evolve slowly because they're not really technologies.<br>Languages are notation. A program is a formal description of <br>the problem you want a computer to solve for you. So the rate<br>of evolution in programming languages is more like the<br>rate of evolution in mathematical notation than, say,<br>transportation or communications.<br>Mathematical notation does evolve, but not with the giant<br>leaps you see in technology.<br>Whatever computers are made of in a hundred years, it seems <br>safe to predict they will be much faster than<br>they are now. If Moore's Law continues to put out, they will be 74<br>quintillion (73,786,976,294,838,206,464) times faster. That's kind of<br>hard to imagine. And indeed, the most likely prediction in the<br>speed department may be that Moore's Law will stop working.<br>Anything that is supposed to double every eighteen months seems<br>likely to run up against some kind of fundamental limit eventually.<br>But I have no trouble believing that computers will be very much<br>faster. Even if they only end up being a paltry million<br>times faster, that should change the ground rules for programming<br>languages substantially. Among other things, there<br>will be more room for what<br>would now be considered slow languages, meaning languages<br>that don't yield very efficient code.<br>And yet some applications will still demand speed.<br>Some of the problems we want to solve with<br>computers are created by computers; for example, the<br>rate at which you have to process video images depends<br>on the rate at which another computer can<br>generate them. And there is another class of problems<br>which inherently have an unlimited capacity to soak up cycles:<br>image rendering, cryptography, simulations.<br>If some applications can be increasingly inefficient while<br>others continue to demand all the speed the hardware can<br>deliver, faster computers will mean that languages have<br>to cover an ever wider range of efficiencies. We've seen<br>this happening already. Current implementations of some<br>popular new languages are shockingly wasteful by the<br>standards of previous decades.<br>This isn't just something that happens with programming<br>languages. It's a general historical trend. As technologies improve,<br>each generation can do things that the previous generation<br>would have considered wasteful. People thirty years ago would<br>be astonished at how casually we make long distance phone calls.<br>People a hundred years ago would be even more astonished that <br>a package would one day travel from Boston to New York via Memphis.<br>I can already tell you what's going to happen to all those extra<br>cycles that faster hardware is going to give us in the <br>next hundred years. They're nearly all going to be wasted.<br>I learned to program when computer power was scarce.<br>I can remember taking all the spaces out of my Basic programs<br>so they would fit into the memory of a 4K TRS-80. The<br>thought of all this stupendously inefficient software<br>burning up cycles doing the same thing over and over seems<br>kind of gross to me. But I think my intuitions here are wrong. I'm<br>like someone who grew up poor, and can't bear to spend money<br>even for something important, like going to the doctor.<br>Some kinds of waste really are disgusting. SUVs, for example, would<br>arguably be gross even if they ran on a fuel which would never<br>run out and generated no pollution. SUVs are gross because they're<br>the solution to a gross problem. (How to make minivans look more<br>masculine.)<br>But not all waste is bad. Now that we have the infrastructure<br>to support it, counting the minutes of your long-distance<br>calls starts to seem niggling. If you have the<br>resources, it's more elegant to think of all phone calls as<br>one kind of thing, no matter where the other person is.<br>There's good waste, and bad waste. I'm interested<br>in good waste-- the kind where, by spending more, we can get <br>simpler designs. How will we take advantage of the opportunities<br>to waste cycles that we'll get from new, faster hardware?<br>The desire for speed is so deeply engrained in us, with <br>our puny computers, that it will take a conscious effort<br>to overcome it. In language design, we should be consciously seeking out<br>situations where we can trade efficiency for even the<br>smallest increase in convenience.<br>Most data structures exist because of speed. For example,<br>many languages today have both strings and lists. Semantically, strings<br>are more or less a subset of lists in which the elements are<br>characters. So why do you need a separate data type?<br>You don't, really. Strings only<br>exist for efficiency. But it's lame to clutter up the semantics<br>of the language with hacks to make programs run faster.<br>Having strings in a language seems to be a case of<br>premature optimization.<br>If we think of the core of a language as a set of axioms, <br>surely it's gross to have additional axioms that add no expressive<br>power, simply for the sake of efficiency. Efficiency is<br>important, but I don't think that's the right way to get it.<br>The right way to solve that problem, I think, is to separate<br>the meaning of a program from the implementation details. <br>Instead of having both lists and strings, have just lists,<br>with some way to give the compiler optimization advice that <br>will allow it to lay out strings as contiguous bytes if<br>necessary.<br>Since speed doesn't matter in most of a program, you won't<br>ordinarily need to bother with<br>this sort of micromanagement.<br>This will be more and more true as computers get faster.<br>Saying less about implementation should also make programs<br>more flexible.<br>Specifications change while a program is being written, and this is not<br>only inevitable, but desirable.<br>The word "essay" comes<br>from the French verb "essayer", which means "to try".<br>An essay, in the original sense, is something you<br>write to try to figure something out. This happens in<br>software too. I think some of the best programs were essays,<br>in the sense that the authors didn't know when they started<br>exactly what they were trying to write.<br>Lisp hackers already know about the value of being flexible<br>with data structures. We tend to write the first version of<br>a program so that it does everything with lists. These<br>initial versions can be so shockingly inefficient that it<br>takes a conscious effort not to think about what they're<br>doing, just as, for me at least, eating a steak requires a<br>conscious effort not to think where it came from.<br>What programmers in a hundred years will be looking for, most of<br>all, is a language where you can throw together an unbelievably<br>inefficient version 1 of a program with the least possible<br>effort. At least, that's how we'd describe it in present-day<br>terms. What they'll say is that they want a language that's<br>easy to program in.<br>Inefficient software isn't gross. What's gross is a language<br>that makes programmers do needless work. Wasting programmer time<br>is the true inefficiency, not wasting machine time. This will<br>become ever more clear as computers get faster.<br>I think getting rid of strings is already something we<br>could bear to think about. We did it in [Arc](https://paulgraham.com/arc.html), and it seems<br>to be a win; some operations that would be awkward to<br>describe as regular expressions can be described<br>easily as recursive functions.<br>How far will this flattening of data structures go? I can think<br>of possibilities that shock even me, with my conscientiously broadened<br>mind. Will we get rid of arrays, for example? After all, they're<br>just a subset of hash tables where the keys are vectors of<br>integers. Will we replace hash tables themselves with lists?<br>There are more shocking prospects even than that. The Lisp<br>that McCarthy described in 1960, for example, didn't<br>have numbers. Logically, you don't need to have a separate notion<br>of numbers, because you can represent them as lists: the integer<br>n could be represented as a list of n elements. You can do math this<br>way. It's just unbearably inefficient.<br>No one actually proposed implementing numbers as lists in<br>practice. In fact, McCarthy's 1960 paper was not, at the time,<br>intended to be implemented at all. It was a [theoretical exercise](https://paulgraham.com/rootsoflisp.html),<br>an attempt to create a more elegant alternative to the Turing<br>Machine. When someone did, unexpectedly, take this paper and<br>translate it into a working Lisp interpreter, numbers certainly<br>weren't represented as lists; they were represented in binary,<br>as in every other language.<br>Could a programming language go so far as to get rid of numbers<br>as a fundamental data type? I ask this not so much as a serious<br>question as as a way to play chicken with the future. It's like<br>the hypothetical case of an irresistible force meeting an <br>immovable object-- here, an unimaginably inefficient<br>implementation meeting unimaginably great resources.<br>I don't see why not. The future is pretty long. If there's<br>something we can do to decrease the number of axioms in the core<br>language, that would seem to be the side to bet on as t approaches<br>infinity. If the idea still seems unbearable in a hundred years,<br>maybe it won't in a thousand.<br>Just to be clear about this, I'm not proposing that all numerical<br>calculations would actually be carried out using lists. I'm proposing<br>that the core language, prior to any additional notations about<br>implementation, be defined this way. In practice any program<br>that wanted to do any amount of math would probably represent<br>numbers in binary, but this would be an optimization, not part of<br>the core language semantics.<br>Another way to burn up cycles is to have many layers of<br>software between the application and the hardware. This too is<br>a trend we see happening already: many recent languages are<br>compiled into byte code. Bill Woods once told me that,<br>as a rule of thumb, each layer of interpretation costs a<br>factor of 10 in speed. This extra cost buys you flexibility.<br>The very first version of Arc was an extreme case of this sort<br>of multi-level slowness, with corresponding benefits. It<br>was a classic "metacircular" interpreter written<br>on top of Common Lisp, with a definite family resemblance<br>to the eval function defined in McCarthy's original Lisp paper.<br>The whole thing was only a couple hundred lines of<br>code, so it was very easy to understand and change. The <br>Common Lisp we used, CLisp, itself runs on top<br>of a byte code interpreter. So here we had two levels of<br>interpretation, one of them (the top one) shockingly inefficient,<br>and the language was usable. Barely usable, I admit, but<br>usable.<br>Writing software as multiple layers is a powerful technique<br>even within applications. Bottom-up programming means writing<br>a program as a series of layers, each of which serves as a<br>language for the one above. This approach tends to yield<br>smaller, more flexible programs. It's also the best route to <br>that holy grail, reusability. A language is by definition<br>reusable. The more<br>of your application you can push down into a language for writing<br>that type of application, the more of your software will be <br>reusable.<br>Somehow the idea of reusability got attached<br>to object-oriented programming in the 1980s, and no amount of<br>evidence to the contrary seems to be able to shake it free. But<br>although some object-oriented software is reusable, what makes<br>it reusable is its bottom-upness, not its object-orientedness.<br>Consider libraries: they're reusable because they're language,<br>whether they're written in an object-oriented style or not.<br>I don't predict the demise of object-oriented programming, by the<br>way. Though I don't think it has much to offer good programmers,<br>except in certain specialized domains, it is irresistible to <br>large organizations. Object-oriented programming<br>offers a sustainable way to write spaghetti code. It lets you accrete<br>programs as a series of patches.<br>Large organizations<br>always tend to develop software this way, and I expect this<br>to be as true in a hundred years as it is today.<br>As long as we're talking about the future, we had better<br>talk about parallel computation, because that's where this <br>idea seems to live. That is, no matter when you're talking, parallel<br>computation seems to be something that is going to happen<br>in the future.<br>Will the future ever catch up with it? People have been<br>talking about parallel computation as something imminent <br>for at least 20<br>years, and it hasn't affected programming practice much so far.<br>Or hasn't it? Already<br>chip designers have to think about it, and so must<br>people trying to write systems software on multi-cpu computers.<br>The real question is, how far up the ladder of abstraction will<br>parallelism go?<br>In a hundred years will it affect even application programmers? Or<br>will it be something that compiler writers think about, but<br>which is usually invisible in the source code of applications?<br>One thing that does seem likely is that most opportunities for<br>parallelism will be wasted. This is a special case of my more <br>general prediction that most of the extra computer power we're<br>given will go to waste. I expect that, as with the stupendous<br>speed of the underlying hardware, parallelism will be something<br>that is available if you ask for it explicitly, but ordinarily<br>not used. This implies that the kind of parallelism we have in<br>a hundred years will not, except in special applications, be<br>massive parallelism. I expect for<br>ordinary programmers it will be more like being able to fork off<br>processes that all end up running in parallel.<br>And this will, like asking for specific implementations of data<br>structures, be something that you do fairly late in the life of a<br>program, when you try to optimize it. Version 1s will ordinarily<br>ignore any advantages to be got from parallel computation, just<br>as they will ignore advantages to be got from specific representations<br>of data.<br>Except in special kinds of applications, parallelism won't<br>pervade the programs that are written in a hundred years. It would be<br>premature optimization if it did.<br>How many programming languages will there<br>be in a hundred years? There seem to be a huge number of new<br>programming languages lately. Part of the reason is that<br>faster hardware has allowed programmers to make different<br>tradeoffs between speed and convenience, depending on the<br>application. If this is a real trend, the hardware we'll <br>have in a hundred years should only increase it.<br>And yet there may be only a few widely-used languages in a<br>hundred years. Part of the reason I say this<br>is optimism: it seems that, if you did a really good job,<br>you could make a language that was ideal for writing a <br>slow version 1, and yet with the right optimization advice<br>to the compiler, would also yield very fast code when necessary.<br>So, since I'm optimistic, I'm going to predict that despite<br>the huge gap they'll have between acceptable and maximal<br>efficiency, programmers in a hundred years will have languages <br>that can span most of it.<br>As this gap widens, profilers will become increasingly important.<br>Little attention is paid to profiling now. Many people still<br>seem to believe that the way to get fast applications is to<br>write compilers that generate fast code. As the gap between <br>acceptable and maximal performance widens, it will become<br>increasingly clear that the way to get fast applications is <br>to have a good guide from one to the other.<br>When I say there may only be a few languages, I'm not including<br>domain-specific "little languages". I think such embedded languages<br>are a great idea, and I expect them to proliferate. But I expect<br>them to be written as thin enough skins that users can see<br>the general-purpose language underneath.<br>Who will design the languages of the future? One of the most exciting<br>trends in the last ten years has been the rise of open-source <br>languages like Perl, Python, and Ruby.<br>Language design is being taken over by hackers. The results<br>so far are messy, but encouraging. There are some stunningly <br>novel ideas in Perl, for example. Many are stunningly bad, but<br>that's always true of ambitious efforts. At its current rate<br>of mutation, God knows what Perl might evolve into in a hundred<br>years.<br>It's not true that those who can't do, teach (some of the best<br>hackers I know are professors), but it is true that there are a<br>lot of things that those who teach can't do. [Research](https://paulgraham.com/desres.html) imposes<br>constraining caste restrictions. In any academic<br>field there are topics that are ok to work on and others that<br>aren't. Unfortunately the distinction between acceptable and<br>forbidden topics is usually based on how intellectual<br>the work sounds when described in research papers, rather than<br>how important it is for getting good results. The extreme case<br>is probably literature; people studying literature rarely <br>say anything that would be of the slightest use to those<br>producing it.<br>Though the situation is better in the sciences,<br>the overlap between the kind of work you're allowed to do and the<br>kind of work that yields good languages is distressingly small.<br>(Olin Shivers has grumbled eloquently<br>about this.) For example, types seem to be an inexhaustible source<br>of research papers, despite the fact that static typing<br>seems to preclude true macros-- without which, in my opinion, no<br>language is worth using.<br>The trend is not merely toward languages being developed<br>as open-source projects rather than "research", but toward<br>languages being designed by the application programmers who need<br>to use them, rather than by compiler writers. This seems a good<br>trend and I expect it to continue.<br>Unlike physics in a hundred years, which is almost necessarily<br>impossible to predict, I think it may be possible in principle<br>to design a language now that would appeal to users in a hundred<br>years.<br>One way to design a language is to just write down the program<br>you'd like to be able to write, regardless of whether there <br>is a compiler that can translate it or hardware that can run it.<br>When you do this you can assume unlimited resources. It seems<br>like we ought to be able to imagine unlimited resources as well<br>today as in a hundred years.<br>What program would one like to write? Whatever is least work.<br>Except not quite: whatever _would be_ least work if your ideas about<br>programming weren't already influenced by the languages you're <br>currently used to. Such influence can be so pervasive that <br>it takes a great effort to overcome it. You'd think it would<br>be obvious to creatures as lazy as us how to express a program<br>with the least effort. In fact, our ideas about what's possible<br>tend to be so [limited](https://paulgraham.com/avg.html) by whatever language we think in that<br>easier formulations of programs seem very surprising. They're<br>something you have to discover, not something you naturally<br>sink into.<br>One helpful trick here<br>is to use the [length](https://paulgraham.com/power.html) of the program as an approximation for<br>how much work it is to write. Not the length in characters,<br>of course, but the length in distinct syntactic elements-- basically,<br>the size of the parse tree. It may not be quite true that<br>the shortest program is the least work to write, but it's<br>close enough that you're better off aiming for the solid<br>target of brevity than the fuzzy, nearby one of least work.<br>Then the algorithm for language design becomes: look at a program<br>and ask, is there any way to write this that's shorter?<br>In practice, writing programs in an imaginary hundred-year<br>language will work to varying degrees depending<br>on how close you are to the core. Sort routines you can<br>write now. But it would be<br>hard to predict now what kinds of libraries might be needed in<br>a hundred years. Presumably many libraries will be for domains that<br>don't even exist yet. If SETI@home works, for example, we'll <br>need libraries for communicating with aliens. Unless of course<br>they are sufficiently advanced that they already communicate<br>in XML.<br>At the other extreme, I think you might be able to design the<br>core language today. In fact, some might argue that it was already<br>mostly designed in 1958.<br>If the hundred year language were available today, would we<br>want to program in it? One way to answer this question is to<br>look back. If present-day programming languages had been available<br>in 1960, would anyone have wanted to use them?<br>In some ways, the answer is no. Languages today assume<br>infrastructure that didn't exist in 1960. For example, a language<br>in which indentation is significant, like Python, would not<br>work very well on printer terminals. But putting such problems<br>aside-- assuming, for example, that programs were all just<br>written on paper-- would programmers of the 1960s have liked<br>writing programs in the languages we use now?<br>I think so.<br>Some of the less imaginative ones,<br>who had artifacts of early languages built into their ideas of <br>what a program was, might have had trouble. (How can you manipulate<br>data without doing pointer arithmetic? How can you implement <br>flow charts without gotos?) But I think the smartest programmers<br>would have had no trouble making the most of present-day<br>languages, if they'd had them.<br>If we had the hundred-year language now, it would at least make a<br>great pseudocode. What about using it to write software? <br>Since the hundred-year language<br>will need to generate fast code for some applications, presumably<br>it could generate code efficient enough to run acceptably well<br>on our hardware. We might have to give more optimization advice<br>than users in a hundred years, but it still might be a net win.<br>Now we have two ideas that, if you combine them, suggest interesting<br>possibilities: (1) the hundred-year language could, in principle, be<br>designed today, and (2) such a language, if it existed, might be good to<br>program in today. When you see these ideas laid out like that,<br>it's hard not to think, why not try writing the hundred-year language<br>now?<br>When you're working on language design, I think it is good to<br>have such a target and to keep it consciously in mind. When you<br>learn to drive, one of the principles they teach you is to<br>align the car not by lining up the hood with the stripes painted<br>on the road, but by aiming at some point in the distance. Even<br>if all you care about is what happens in the next ten feet, this<br>is the right answer. I<br>think we can and should do the same thing with programming languages.<br>**Notes**<br>I believe Lisp Machine Lisp was the first language to embody<br>the principle that declarations (except those of dynamic variables)<br>were merely optimization advice,<br>and would not change the meaning of a correct program. Common Lisp<br>seems to have been the first to state this explicitly.<br>**Thanks** to Trevor Blackwell, Robert Morris, and Dan Giffin for<br>reading drafts of this, and to Guido van Rossum, Jeremy Hylton, and the<br>rest of the Python crew for inviting me to speak at PyCon. |

|     |
| --- |
| * * *

|     |
| --- |
| ![](http://www.virtumundo.com/images/spacer.gif)<br>You'll find this essay and 14 others in<br>[**_Hackers & Painters_**](https://paulgraham.com/hackpaint.html).<br>![](http://www.virtumundo.com/images/spacer.gif) | | |
