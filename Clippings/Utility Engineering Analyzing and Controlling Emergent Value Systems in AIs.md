---
title: "Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs"
source: "https://arxiv.org/abs/2502.08640"
author:
  - "[[Mantas Mazeika]]"
  - "[[Xuwang Yin]]"
  - "[[Rishub Tamirisa]]"
  - "[[Jaehyuk Lim]]"
  - "[[Bruce W. Lee]]"
  - "[[Richard Ren]]"
  - "[[Long Phan]]"
  - "[[Norman Mu]]"
  - "[[Adam Khoja]]"
  - "[[Oliver Zhang]]"
published:
created: 2025-09-05
description: "Abstract page for arXiv paper 2502.08640: Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs"
tags:
  - "clippings"
---
\[Submitted on 12 Feb 2025 ([v1](https://arxiv.org/abs/2502.08640v1)), last revised 19 Feb 2025 (this version, v2)\]

## Title:Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs

Authors:, , , , , , , , , ,

[View PDF](https://arxiv.org/pdf/2502.08640)

> Abstract:As AIs rapidly advance and become more agentic, the risk they pose is governed not only by their capabilities but increasingly by their propensities, including goals and values. Tracking the emergence of goals and values has proven a longstanding problem, and despite much interest over the years it remains unclear whether current AIs have meaningful values. We propose a solution to this problem, leveraging the framework of utility functions to study the internal coherence of AI preferences. Surprisingly, we find that independently-sampled preferences in current LLMs exhibit high degrees of structural coherence, and moreover that this emerges with scale. These findings suggest that value systems emerge in LLMs in a meaningful sense, a finding with broad implications. To study these emergent value systems, we propose utility engineering as a research agenda, comprising both the analysis and control of AI utilities. We uncover problematic and often shocking values in LLM assistants despite existing control measures. These include cases where AIs value themselves over humans and are anti-aligned with specific individuals. To constrain these emergent value systems, we propose methods of utility control. As a case study, we show how aligning utilities with a citizen assembly reduces political biases and generalizes to new scenarios. Whether we like it or not, value systems have already emerged in AIs, and much work remains to fully understand and control these emergent representations.

| Comments: | Website: [this https URL](https://www.emergent-values.ai/) |
| --- | --- |
| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY) |
| Cite as: | [arXiv:2502.08640](https://arxiv.org/abs/2502.08640) \[cs.LG\] |
|  | (or [arXiv:2502.08640v2](https://arxiv.org/abs/2502.08640v2) \[cs.LG\] for this version) |
|  | [https://doi.org/10.48550/arXiv.2502.08640](https://doi.org/10.48550/arXiv.2502.08640)  arXiv-issued DOI via DataCite |

## Submission history

From: Mantas Mazeika \[[view email](https://arxiv.org/show-email/0455cbdd/2502.08640)\]  
**[\[v1\]](https://arxiv.org/abs/2502.08640v1)** Wed, 12 Feb 2025 18:55:43 UTC (2,515 KB)  
**\[v2\]** Wed, 19 Feb 2025 06:48:30 UTC (2,538 KB)  

## Bibliographic and Citation Tools

Bibliographic Explorer *([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*

Connected Papers *([What is Connected Papers?](https://www.connectedpapers.com/about))*

Litmaps *([What is Litmaps?](https://www.litmaps.co/))*

scite Smart Citations *([What are Smart Citations?](https://www.scite.ai/))*

## Code, Data and Media Associated with this Article

alphaXiv *([What is alphaXiv?](https://alphaxiv.org/))*

CatalyzeX Code Finder for Papers *([What is CatalyzeX?](https://www.catalyzex.com/))*

DagsHub *([What is DagsHub?](https://dagshub.com/))*

Gotit.pub *([What is GotitPub?](http://gotit.pub/faq))*

Hugging Face *([What is Huggingface?](https://huggingface.co/huggingface))*

Papers with Code *([What is Papers with Code?](https://paperswithcode.com/))*

ScienceCast *([What is ScienceCast?](https://sciencecast.org/welcome))*

## Demos

Replicate *([What is Replicate?](https://replicate.com/docs/arxiv/about))*

Hugging Face Spaces *([What is Spaces?](https://huggingface.co/docs/hub/spaces))*

TXYZ.AI *([What is TXYZ.AI?](https://txyz.ai/))*

## arXivLabs: experimental projects with community collaborators

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.

Have an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).

[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2502.08640) | [Disable MathJax](https://arxiv.org/abs/) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html))